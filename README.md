# Deep Learning Nanodegree Portfolio

This repository contains my work for Udacity's Deep Learning Nanodegree program, August 2017 cohort.

**Related:** My [Data Analyst Nanodegree Portfolio](https://github.com/seifip/udacity-data-analyst-nanodegree).

### Projects:

* _Project 1:_ [Your First Neural Network](https://github.com/seifip/udacity-deep-learning-nanodegree/tree/master/!P1%20-%20First%20Neural%20Network): Implement a neural network in Numpy to predict bike rentals.
* **WIP** _Project 2:_ [Image classification](https://github.com/seifip/udacity-deep-learning-nanodegree/tree/master/!P2%20-%20Image%20Classification): Build a convolutional neural network with TensorFlow to classify CIFAR-10 images.
* **WIP** _Project 3:_ [Text Generation](https://github.com/seifip/udacity-deep-learning-nanodegree/tree/master/!P3%20-%20TV%20Script%20Generation): Train a recurrent neural network on scripts from The Simpson's (copyright Fox) to generate new scripts.
* **WIP** _Project 4:_ [Machine Translation](https://github.com/seifip/udacity-deep-learning-nanodegree/tree/master/!P4%20-%20Language%20Translation): Train a sequence to sequence network for English to French translation (on a simple dataset)
* **WIP** _Project 5:_ [Face Generation](https://github.com/seifip/udacity-deep-learning-nanodegree/tree/master/!P5%20-%20Face%20Generation): Use a DCGAN on the CelebA dataset to generate images of novel and realistic human faces.

## Tutorials:

* [Sentiment Analysis with Numpy]() leads you through building a sentiment analysis model, predicting if some text is positive or negative.
* [Intro to TensorFlow](): Starting building neural networks with Tensorflow.
* [Weight Intialization](): Explore how initializing network weights affects performance.
* [Autoencoders](): Build models for image compression and denoising, using feed-forward and convolution networks in TensorFlow.
* [Transfer Learning (ConvNet)](). In practice, most people don't train their own large networkd on huge datasets, but use pretrained networks such as VGGnet. Here you'll use VGGnet to classify images of flowers without training a network on the images themselves.
* [Intro to Recurrent Networks (Character-wise RNN)](): Recurrent neural networks are able to use information about the sequence of data, such as the sequence of characters in text.
* [Embeddings (Word2Vec)](): Implement the Word2Vec model to find semantic representations of words for use in natural language processing.
* [Sentiment Analysis RNN](): Implement a recurrent neural network that can predict if a text sample is positive or negative.
* [Tensorboard](): Use TensorBoard to visualize the network graph, as well as how parameters change through training.
* [Reinforcement Learning (Q-Learning)](): Implement a deep Q-learning network to play a simple game from OpenAI Gym.
* [Sequence to sequence](): Implement a sequence-to-sequence recurrent network.
* [Batch normalization](): Learn how to improve training rates and network stability with batch normalizations.
* [Generative Adversatial Network on MNIST](): Train a simple generative adversarial network on the MNIST dataset.
* [Deep Convolutional GAN (DCGAN)](): Implement a DCGAN to generate new images based on the Street View House Numbers (SVHN) dataset.
* [Intro to TFLearn](): A couple introductions to a high-level library for building neural networks.

## Courses taken:
* Neural Networks
* Deep Neural Networks
* Convolutional Neural Networks (CNNs)
* Recurrent Neural Networks (RNNs)
* Generative Adversarial Networks (GANs)

## Skills acquired:
* TensorFlow, Keras
* Cloud computing (AWS, GCE, FloydHub)
* Weight initialization
* Sentiment Analysis
* Image Classification
* Image Generation
* Autoencoders
* Transfer Learning
* Embeddings and Word2vec
* Style Transfer
* Text summarization
* Sequence to sequence
* Dynamic memory networks
* Machine translation
* Semi-Supervised Learning
